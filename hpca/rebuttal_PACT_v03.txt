We are thankful to the reviewers for their valuable comments. 
This paper extensively highlights the limitations of the state-of-the-art OS scheduling approaches (e.g. GTS) and demonstrates how to tackle them by using task-based parallel runtime systems. This work is novel as it demonstrates for the first time how the scheduling responsibility should be distributed across different layers of the software stack, not just the OS layer. To our knowledge, none of the previous works on asymmetric multi-core architectures has provided such a thorough and reliable performance, energy and power evaluation on real HPC applications.

The GTS approach is the most sophisticated dynamic OS job scheduling approach on the market and is deployed with most big.LITTLE boards. GTS is commercially available and is provided by ARM to offer performance and energy efficiency. GTS provides good results in multi-programmed workload scenarios, as has been shown in some publications from ARM/Samsung. However, our findings show that GTS is not ready to handle correctly workloads with a single parallel application running on the asymmetric system. We are not aware of any publication that highlights this limitation. In contrast, we prove that task-based programming models achieve much better results than GTS. We truly believe that this is a novel contribution in the community.

Additionally, our work exploits the use of assistant cores for the runtime activity. The findings of this section propose modifications on the task-based scheduling approach so that one core is devoted for the execution of the sequential code regions and of the runtime overheads. This part of the paper shows that applications with high sequential code regions benefit from big assistant core, while most of the applications benefit when the runtime is executed on the first available core of the system.

We consider that this paper is a valuable piece of work that provides researchers with useful results for building and utilizing future asymmetric multi-core systems. The use of real hardware and real HPC workloads make this paper a reliable and unique illustration of the efficient exploitation of asymmetric systems, taking into account the performance and energy tradeoffs.

Finally, qualifying the paper as "Published before or openly commercialized" (Reviewer C) without giving any reference to a publication or product is really unfair and arbitrary. We completely disagree with this claim (as we have explained above) and we ask the reviewer either to provide such references or change his qualification.

We agree that a few parts of the text could be improved so that they do not raise questions to the reader. Firstly, as reviewerA pointed out, we should clarify the phrase of page2 to show that the meaning is exactly as understood by the reviewer. Moreover, we plan to explain that the design of fluidanimate and facesim benchmarks limits their results to core counts that are either multiples or powers of 2 respectively. Also, reviewerA is right that Figure 2 is not used in the evaluation section and we will consider removing it. 

Moreover, we plan to highlight why the set of benchmarks used from the PARSECSs suite is considered representative, as reviewerD highlights. The reason for omitting freqmine is that it follows the same parallelization strategies as blackscholes and streamcluster and for x264 is that it is another pipelined parallel application like ferret and dedup. 

As the reviewerD notes, thanks to this study, we can provide possible optimizations to applications useful for asymmetric multi-cores. We will add this discussion in the conclusions of the paper.
The description of Cluster Switching and In-Kernel Switching is included to provide a historical perspective of OS schedulers for asymmetric multi-cores. However, we agree that this description can be significantly reduced. Finally we will clarify that GTS does not use application-specific techniques as it only monitors CPU utilization. In the case of bodytrack, I/O tasks (which have lower CPU utilization) are executed on the little cores, which are in the critical path of the application.
